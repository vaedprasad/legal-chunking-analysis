from copy import deepcopy
from functools import wraps
from typing import Any, Callable, Dict, Optional

import openai
from dotenv import dotenv_values
from tenacity import (
    retry,
    retry_if_exception_type,
    stop_after_attempt,
    wait_exponential,
)

from constants import DEFAULT_OPENAI_PARAMS, OPENAI_SECRET_NAME, USER
from exceptions import FriendlyException


def handle_openai_errors(func: Callable) -> Callable:
    @wraps(func)
    def wrapper(instance: "OpenAIPromptService", *args: Any, **kwargs: Any) -> Any:
        try:
            return func(instance, *args, **kwargs)

        except openai.AuthenticationError as e:
            raise FriendlyException(
                detail="Unauthorized to run inference on OpenAI for that model.",
                user_friendly_message="OpenAI connection unauthorized.",
                how_to_fix="Please ensure that the OpenAI API key has correctly been set in your .env file.",
            ) from e

        except openai.RateLimitError as e:
            rsp_err = str(e).lower()
            if "exceeded your current quota" in rsp_err:
                err_msg = "OpenAI credit limit reached."
                raise FriendlyException(
                    detail=err_msg,
                    user_friendly_message=err_msg,
                    how_to_fix="Please add more credit to your OpenAI account and try again.",
                ) from e
            elif "currently overloaded" in rsp_err:
                err_msg = "OpenAI servers are experiencing high traffic and are unable to process your request at the moment."
                raise FriendlyException(
                    detail=err_msg,
                    user_friendly_message=err_msg,
                    how_to_fix="Please wait a few minutes and try again. Check the status of OpenAI's servers at https://status.openai.com/.",
                ) from e
            else:
                err_msg = "OpenAI rate limit reached."
                raise FriendlyException(
                    detail=err_msg,
                    user_friendly_message=err_msg,
                    how_to_fix="Please wait a few minutes and try again. If this problem persists, consider requesting a higher rate limit for your organisation from OpenAI.",
                ) from e
        except openai.InternalServerError as e:
            err_msg = "OpenAI's inference service is down."
            raise FriendlyException(
                detail=err_msg,
                user_friendly_message=err_msg,
                how_to_fix="Please try again later, check https://status.openai.com/ for updates.",
            ) from e
        except Exception as e:
            raise FriendlyException(
                detail="We ran into a problem making that request to OpenAI.",
                user_friendly_message="OpenAI is encountering issues.",
                how_to_fix="Please try again later, check https://status.openai.com/ for updates.",
            ) from e

    return wrapper


openai_retry = retry(
    retry=retry_if_exception_type(
        (
            openai.APIError,
            openai.APITimeoutError,
            openai.APIConnectionError,
            openai.RateLimitError,
            openai.InternalServerError,
        )
    ),
    stop=stop_after_attempt(4),
    reraise=True,
    wait=wait_exponential(multiplier=1.5),
)


class OpenAIPromptService:
    """
    This service interfaces with the OpenAI API to generate chat completions.

    Examples
    --------
    >>> service = OpenAIPromptService()
    >>> service.run_prompt("Hello, world!", {"model": "gpt-4-0125-preview"})
    'Hello, world! How can I assist you today?'
    """

    def __init__(self) -> None:
        config = dotenv_values(".env")
        self.client = openai.OpenAI(api_key=config[OPENAI_SECRET_NAME])

    def run_prompt(
        self, prompt: str, model_config: Optional[Dict[str, Any]] = None
    ) -> str:
        """
        Executes a given prompt using the specified model configuration and returns the text response.

        Parameters
        ----------
        prompt : str
            The text prompt to send to the model.
        model_config : Dict[str, Any]
            A dictionary containing configuration parameters for the model, such as model type and additional settings.

        Returns
        -------
        str
            The text response generated by the model.

        Examples
        --------
        >>> model_config = {"model": "gpt-3.5-turbo"}
        >>> service = OpenAIPromptService()
        >>> response = service.run_prompt("What is the capital of France?", model_config)
        >>> print(response)
        'The capital of France is Paris.'
        """
        full_output = self.run_prompt_with_full_output(
            prompt=prompt, model_config=model_config
        )
        return full_output.choices[0].message.content

    def run_prompt_with_full_output(
        self, prompt: str, model_config: Optional[Dict[str, Any]] = None
    ) -> Any:
        """
        Executes a prompt and returns the full response object from the model.

        Parameters
        ----------
        prompt : str
            The text prompt to send to the model.
        model_config : Optional[Dict[str, Any]]
            A dictionary containing configuration parameters for the model. Defaults to None

        Returns
        -------
        Any
            The full response object from the model, which may include additional metadata.

        Examples
        --------
        >>> model_config = {"model": "gpt-3.5-turbo"}
        >>> service = OpenAIPromptService()
        >>> full_output = service.run_prompt_with_full_output("What is the capital of France?", model_config)
        >>> print(full_output)
        {'text': 'The capital of France is Paris.', 'other_info': '...'}
        """
        config = deepcopy(DEFAULT_OPENAI_PARAMS)
        if model_config is not None:
            config.update(model_config)
        return self._create_completion(prompt, **config)

    @handle_openai_errors
    @openai_retry
    def _create_completion(self, prompt: str, **kwargs: Any) -> Any:
        """
        Internal method to create a completion request to the OpenAI.

        Parameters
        ----------
        prompt : str
            The prompt text to be sent to the model.
        **kwargs : Any
            Additional keyword arguments including model configurations.

        Returns
        -------
        Any
            The response from the model, which could be a text completion or a more complex object depending on the model.
        """
        return self.client.chat.completions.create(
            messages=[dict(role=USER, content=prompt)], **kwargs
        )
